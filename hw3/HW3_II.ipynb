{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3: PySpark - II\n",
    "### CS186, UC Berkeley, Spring 2016\n",
    "### Due: Thursday Feb 25, 2016, 11:59 PM\n",
    "### Note: **This homework is to be done individually!  Do not modify any existing method signatures.**\n",
    "### **This is the second of two .ipynb files in this homework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## On some computers it may be possible to run this lab \n",
    "## locally by using this script; you will need to run\n",
    "## this each time you start the notebook.\n",
    "## You do not need to run this on inst machines.\n",
    "\n",
    "# from local_install import setup_environment\n",
    "# setup_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from utils import SparkContext as sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from utils import CleanRDD\n",
    "from utils import tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: CacheMap\n",
    "\n",
    "In this part, we'll construct an rdd that is backed by a `ClockMap` and will behave like `rdd.map(func)`.  \n",
    "First, implement the `ClockMap` class so that it maintains a cache (of limited `cacheSize`) using the clock replacement policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * BEGIN STUDENT CODE *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ClockMap:\n",
    "    \n",
    "    def __init__(self, cacheSize, func):\n",
    "        \"\"\"\n",
    "        Do not change existing variables.\n",
    "        [Optional] You are free to add additional items and methods.\n",
    "        \"\"\"\n",
    "        self.cacheSize = cacheSize\n",
    "        self.fn = func\n",
    "        self._p = 0 # pointer\n",
    "        self._increments = 0\n",
    "        self._miss_count = 0\n",
    "        self.buffers = [[None, 0] for x in range(cacheSize)]\n",
    "        self.items_to_index = {}\n",
    "        self.cleaning_map = {} #handle map deletions\n",
    "        \n",
    "    def _increment(self):\n",
    "        \"\"\"\n",
    "        Do not change this method.\n",
    "        Updates the clock pointer. The modulo maintains the clock nature.\n",
    "        \"\"\"\n",
    "        self._increments += 1\n",
    "        self._p = (self._p + 1) % self.cacheSize\n",
    "\n",
    "    def __getitem__(self, k):\n",
    "        \"\"\"\n",
    "        Returns func(k) using the buffer to cache limited results.\n",
    "        \n",
    "        :param k: Value to be evaluated\n",
    "        \n",
    "        >>> clock = ClockMap(4, lambda x: x ** 2)\n",
    "        >>> clock[4]\n",
    "        16\n",
    "        >>> clock[3]\n",
    "        9\n",
    "        >>> clock._p\n",
    "        2\n",
    "        \"\"\"\n",
    "        #if key exists inside cache return func(k) don't increment hand, cache hit, set bit\n",
    "        if k in self.items_to_index:\n",
    "            self.buffers[self.items_to_index[k]][1] = 1\n",
    "            return self.buffers[self.items_to_index[k]][0]\n",
    "        #otherwise cache miss\n",
    "        else:\n",
    "            #if there is free space\n",
    "            free_space_dest = len(self.items_to_index)\n",
    "            if free_space_dest < self.cacheSize:\n",
    "                self.items_to_index[k] = free_space_dest\n",
    "                self.buffers[self.items_to_index[k]] = [self.fn(k), 1]\n",
    "                self.cleaning_map[free_space_dest] = k\n",
    "                self._increment()\n",
    "            #no free space\n",
    "            else:\n",
    "                #go around all the buffers until a replacement is found\n",
    "                while self.buffers[self._p][1] != 0:\n",
    "                    self.buffers[self._p][1] = 0\n",
    "                    self._increment()\n",
    "                #once a false buffer hits, delete dictionary location, replace then increment\n",
    "                del self.items_to_index[self.cleaning_map[self._p]]\n",
    "                self.items_to_index[k] = self._p\n",
    "                self.buffers[self._p] = [self.fn(k), 1]\n",
    "                self.cleaning_map[self._p] = k\n",
    "                self._increment()\n",
    "            return self.buffers[self.items_to_index[k]][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now implement `cacheMap`, which will return an rdd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cacheMap(rdd, cacheSize, func):\n",
    "    \"\"\"\n",
    "    Returns an RDD that behaves like rdd.map(func) but\n",
    "    is implemented using the ClockMap.\n",
    "    \n",
    "    :param rdd: Given RDD\n",
    "    :param cacheSize: Number of cache/buffer pages in the ClockMap\n",
    "    :param func: Function to map with\n",
    "    \"\"\"\n",
    "    def clock_cache_generator(index, iterator):\n",
    "        cache_map = ClockMap(cacheSize, func)\n",
    "        for item in iterator:\n",
    "            yield cache_map[item]\n",
    "    \n",
    "    return rdd.mapPartitionsWithIndex(clock_cache_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * END STUDENT CODE *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 9\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "clock = ClockMap(4, lambda x: x ** 2)\n",
    "print clock[4], clock[3]\n",
    "print clock._p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Free test for you!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output should be \n",
    "```\n",
    "16, 9\n",
    "2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: External Algorithms\n",
    "\n",
    "You'll need an understanding of the partitioning step of external hashing, and the divide step of external sorting (recall the lecture on external algorithms)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import ceil # For buck list\n",
    "from utils import *\n",
    "import itertools\n",
    "import bisect\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following are some tools you may want to use (examples use cases included). You should Google the unfamiliar ones!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# itertools.islice\n",
    "generator = (y for y in range(100))\n",
    "test1 = itertools.islice(generator, 5)\n",
    "next(test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# heapq.merge\n",
    "generator1 = (odd for odd in range(100) if odd % 2)\n",
    "generator2 = (even for even in range(100)[::2])\n",
    "key = lambda x: x\n",
    "test2 = heapq.merge([generator1, generator2], key=key, reverse=False)\n",
    "next(test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If we insert 3, it goes to 1\n",
      "If we insert 1, it goes to 0\n",
      "If we insert 4, it goes to 1\n"
     ]
    }
   ],
   "source": [
    "# bisect.bisect_left\n",
    "buckets = [2, 4, 4]\n",
    "print \"If we insert 3, it goes to %d\" % bisect.bisect_left(buckets, 3)\n",
    "print \"If we insert 1, it goes to %d\" % bisect.bisect_left(buckets, 1)\n",
    "print \"If we insert 4, it goes to %d\" % bisect.bisect_left(buckets, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8, 9, 12, 30, 41, 50, 65, 76, 79, 84]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RDD.sample\n",
    "rdd = sc.parallelize(range(100))\n",
    "fraction = 0.1\n",
    "rdd.sample(False, fraction).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# Serializer and os.unlink (Serializer is provided via utils.GeneralTools)\n",
    "generator1 = (odd for odd in range(100) if odd % 2)\n",
    "filename = \"temp\"\n",
    "with open(filename, \"w\") as f:\n",
    "    serializer.dump_stream(generator1, f)\n",
    "\n",
    "with open(filename, \"r\") as f:\n",
    "    stream = serializer.load_stream(f)\n",
    "    print next(stream)\n",
    "\n",
    "os.unlink(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get_used_memory - returns an int in MB\n",
    "get_used_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No need to modify the following function - it should come in handy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sort_dir(partId, n):\n",
    "    \"\"\"\n",
    "    Returns a path for temporary file.\n",
    "\n",
    "    :param n: Unique identification for file\n",
    "    \"\"\"\n",
    "    d = \"tmp/sort/\" + str(partId) + \"/\"\n",
    "    if not os.path.exists(d):\n",
    "        os.makedirs(d)\n",
    "    return os.path.join(d, str(n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * BEGIN STUDENT CODE *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def externalSortStream(iterator, partId=0, reverse=False, keyfunc=None, serial=serializer, limit=10, batch=100):\n",
    "    \"\"\"\n",
    "    Given an iterator, returns an iterator of sorted elements (according to parameters). \n",
    "    \n",
    "    :param iterator: iterator. Expects (Key, Value).\n",
    "    :param keyfunc: function applied on the keykey.\n",
    "    :param reverse: Reverse default ordering if true. (default is ascending; reverse is descending) \n",
    "    :param serializer: See README.\n",
    "    :param limit: memory limit.\n",
    "    :param batch: Number of elements to read at a time.\n",
    "    \"\"\"\n",
    "    \n",
    "    all_runs = [] # can be used to hold a list of iterators\n",
    "    run = [] # used to hold the current run of elements\n",
    "    run_count = 0 #for use as a unique identifier to create files\n",
    "    file_cleanup = [] # use to save file paths to call os.unlink on for clean up\n",
    "    \n",
    "    def load(fileobj):\n",
    "        \"\"\"\n",
    "        Returns a generator object that outputs elements \n",
    "        from a serialized (saved) stream. Closes the file when done.\n",
    "        \n",
    "        :param fileobj: python object file\n",
    "        \"\"\"\n",
    "        for _ in serial.load_stream(fileobj):\n",
    "            yield _\n",
    "        fileobj.close()\n",
    "   \n",
    "    # TODO everywhere below \n",
    "    \n",
    "    #start streaming through iterator\n",
    "    while True:\n",
    "        #Read the stream in batches\n",
    "        c = list(itertools.islice(iterator, batch))\n",
    "        \n",
    "        #If list is empty, done streaming\n",
    "        if not c and not run:\n",
    "            break\n",
    "\n",
    "        #As long as the RAM limit has not been exceeded add the values to run\n",
    "        run.extend(c)\n",
    "            \n",
    "        #If the memory limit has been exceeded sort the run by key and write it to the disk, then \n",
    "        #reopen the written file as an iterator and store it in all_runs\n",
    "        if get_used_memory() > limit or not c:\n",
    "            run.sort(key=lambda x: keyfunc(x[0]), reverse = reverse)\n",
    "            path = get_sort_dir(partId, run_count)\n",
    "            \n",
    "            #Write run to file\n",
    "            fileobj_w = open(path, 'w+')\n",
    "            serial.dump_stream(run, fileobj_w)\n",
    "            fileobj_w.close()\n",
    "            \n",
    "            #Read file as a generator\n",
    "            fileobj_r = open(path, 'r')\n",
    "            os.unlink(path) #Closes once the file stream is fully read and closed\n",
    "            all_runs.append(load(fileobj_r))\n",
    "           \n",
    "            #Clear run\n",
    "            del run[:]\n",
    "            run_count += 1 \n",
    "    \n",
    "    return heapq.merge(all_runs, key=lambda x: keyfunc(x[0]), reverse=reverse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Remember to run the import box above.\n",
    "\n",
    "def partitionByKey(rdd, ascending=True, numPartitions=None, keyfunc=lambda x: x):\n",
    "    \"\"\"        \n",
    "    Uses sampling to partitions the elements by the return value of \n",
    "    keyfunc.\n",
    "\n",
    "    :param ascending: Smallest first.\n",
    "    :param numPartitions: Number of partitions of the returning RDD.\n",
    "    :param keyfunc: function to be applied to the key.\n",
    "    \"\"\"\n",
    "    # Base cases done.\n",
    "\n",
    "    if numPartitions is None:\n",
    "        numPartitions = rdd.getNumPartitions()\n",
    "\n",
    "    if numPartitions == 1:\n",
    "        if rdd.getNumPartitions() > 1:\n",
    "            rdd = rdd.coalesce(1)\n",
    "        return rdd\n",
    "    \n",
    "    # TODO\n",
    "    buckets = getBuckets(rdd, ascending, numPartitions, key)\n",
    "\n",
    "    # 2 cases, either the bucket list is ascending or descending\n",
    "    if ascending:\n",
    "        balanceLoad = lambda x: bisect.bisect_left(buckets, keyfunc(x)) # bisect.bisect_left\n",
    "    else:\n",
    "        balanceLoad = lambda x: len(buckets) - bisect.bisect_left(buckets, keyfunc(x)) - 1\n",
    "    return rdd.partitionBy(numPartitions, balanceLoad)\n",
    "\n",
    "\n",
    "def getBuckets(rdd, ascending=True, numPartitions=None, keyfunc=lambda x: x):\n",
    "    \"\"\"        \n",
    "    [Optional] Returns a list of bucket boundaries of length (numPartitions - 1),\n",
    "    in an order as specfied by the given parameters: ascending, keyfunc. \n",
    "    Bucket boundaries are determined by sampling as specified in the README.\n",
    "\n",
    "    :param ascending: Smallest first.\n",
    "    :param numPartitions: Number of partitions of the returning RDD.\n",
    "    :param keyfunc: function to be applied to the key.\n",
    "    \"\"\"\n",
    "    # Base cases done.\n",
    "    pivots = []\n",
    "    \n",
    "    percent = 1\n",
    "    temp_percent = 10 * numPartitions/rdd.count()\n",
    "    if temp_percent < 1:\n",
    "        percent = temp_percent\n",
    "    get_sample = rdd.sample(False, percent).collect()\n",
    "    get_sample.sort(key=lambda x: keyfunc(x[0]), reverse=(not ascending))\n",
    "    \n",
    "    pivot_count = ceil(len(get_sample)/(numPartitions - 1))\n",
    "    \n",
    "    #Select pivots ranges based on court distributions\n",
    "    count = 1\n",
    "    for key, value in get_sample:\n",
    "        if count % pivot_count == 0:\n",
    "            pivots.append(key)\n",
    "        count += 1      \n",
    "    \n",
    "    return pivots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sortByKey(rdd, ascending=True, numPartitions=None, keyfunc=lambda x: x):\n",
    "    \"\"\"\n",
    "    Returns an RDD after executing an external sort using \n",
    "    functions partitionByKey and externalSortStream. \n",
    "\n",
    "    :param ascending: Smallest first.\n",
    "    :param numPartitions: Number of partitions of the returning RDD.\n",
    "    :param keyFunc: function to be applied to the key.\n",
    "    \"\"\"\n",
    "    def generator(index, iterator):\n",
    "        return externalSortStream(iterator, index, not ascending, keyfunc)\n",
    "    \n",
    "    partitioned_rdd = partitionByKey(rdd, ascending, numPartitions, keyfunc)\n",
    "    \n",
    "    return partitioned_rdd.mapPartitionsWithIndex(generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * END STUDENT CODE *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are tests for `partitionByKey` and `externalSortStream`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(7, 7),\n",
       " (6, 6),\n",
       " (8, 8),\n",
       " (5, 5),\n",
       " (9, 9),\n",
       " (4, 4),\n",
       " (3, 3),\n",
       " (2, 2),\n",
       " (1, 1),\n",
       " (0, 0)]"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_stream = ((i, i) for i in range(100))\n",
    "list(externalSortStream(test_stream, keyfunc=(lambda x: abs(50 - (x ** 2)))))[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your output should be:\n",
    "```\n",
    "[(7, 7),\n",
    " (6, 6),\n",
    " (8, 8),\n",
    " (5, 5),\n",
    " (9, 9),\n",
    " (4, 4),\n",
    " (3, 3),\n",
    " (2, 2),\n",
    " (1, 1),\n",
    " (0, 0)]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<utils.CleanRDD.CleanRDD at 0x7fbd02bb10d0>"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = CleanRDD(sc.parallelize(range(20), 4).map(lambda x: (x * 37 % 6, x ** 3 % 34)))\n",
    "partitionByKey(rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your output should look rather well-distributed. Try forcing a skewed distribution and observe how effective the partitioning is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a test for `sortByKey`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(-99, 99),\n",
       " (-97, 97),\n",
       " (-95, 95),\n",
       " (-93, 93),\n",
       " (-91, 91),\n",
       " (-89, 89),\n",
       " (-87, 87),\n",
       " (-85, 85),\n",
       " (-83, 83),\n",
       " (-81, 81),\n",
       " (-79, 79),\n",
       " (-77, 77),\n",
       " (-75, 75),\n",
       " (-73, 73),\n",
       " (-71, 71),\n",
       " (-69, 69),\n",
       " (-67, 67),\n",
       " (-65, 65),\n",
       " (-63, 63),\n",
       " (-61, 61),\n",
       " (-59, 59),\n",
       " (-57, 57),\n",
       " (-55, 55),\n",
       " (-53, 53),\n",
       " (-51, 51),\n",
       " (-49, 49),\n",
       " (-47, 47),\n",
       " (-45, 45),\n",
       " (-43, 43),\n",
       " (-41, 41),\n",
       " (-39, 39),\n",
       " (-37, 37),\n",
       " (-35, 35),\n",
       " (-33, 33),\n",
       " (-31, 31),\n",
       " (-29, 29),\n",
       " (-27, 27),\n",
       " (-25, 25),\n",
       " (-23, 23),\n",
       " (-21, 21),\n",
       " (-19, 19),\n",
       " (-17, 17),\n",
       " (-15, 15),\n",
       " (-13, 13),\n",
       " (-11, 11),\n",
       " (-9, 9),\n",
       " (-7, 7),\n",
       " (-5, 5),\n",
       " (-3, 3),\n",
       " (-1, 1),\n",
       " (0, 0),\n",
       " (2, 2),\n",
       " (4, 4),\n",
       " (6, 6),\n",
       " (8, 8),\n",
       " (10, 10),\n",
       " (12, 12),\n",
       " (14, 14),\n",
       " (16, 16),\n",
       " (18, 18),\n",
       " (20, 20),\n",
       " (22, 22),\n",
       " (24, 24),\n",
       " (26, 26),\n",
       " (28, 28),\n",
       " (30, 30),\n",
       " (32, 32),\n",
       " (34, 34),\n",
       " (36, 36),\n",
       " (38, 38),\n",
       " (40, 40),\n",
       " (42, 42),\n",
       " (44, 44),\n",
       " (46, 46),\n",
       " (48, 48),\n",
       " (50, 50),\n",
       " (52, 52),\n",
       " (54, 54),\n",
       " (56, 56),\n",
       " (58, 58),\n",
       " (60, 60),\n",
       " (62, 62),\n",
       " (64, 64),\n",
       " (66, 66),\n",
       " (68, 68),\n",
       " (70, 70),\n",
       " (72, 72),\n",
       " (74, 74),\n",
       " (76, 76),\n",
       " (78, 78),\n",
       " (80, 80),\n",
       " (82, 82),\n",
       " (84, 84),\n",
       " (86, 86),\n",
       " (88, 88),\n",
       " (90, 90),\n",
       " (92, 92),\n",
       " (94, 94),\n",
       " (96, 96),\n",
       " (98, 98)]"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = CleanRDD(sc.parallelize(range(100), 4).map(lambda x: (x *((-1) ** x) , x)))\n",
    "sortByKey(rdd, keyfunc=lambda key: key, ascending=False).collect()[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Your output should be:\n",
    "```\n",
    "[(-81, 81),\n",
    " (-83, 83),\n",
    " (-85, 85),\n",
    " (-87, 87),\n",
    " (-89, 89),\n",
    " (-91, 91),\n",
    " (-93, 93),\n",
    " (-95, 95),\n",
    " (-97, 97),\n",
    " (-99, 99)]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 3: PASS - task3ClockMap.txt matched reference output.\n",
      "Task 3: PASS - task3CacheMap.txt matched reference output.\n",
      "Task 4: PASS - task4.txt matched reference output.\n"
     ]
    }
   ],
   "source": [
    "tests.test3ClockMap(ClockMap)\n",
    "tests.test3CacheMap(cacheMap)\n",
    "tests.test4(sortByKey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
